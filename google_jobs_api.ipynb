{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "from config import serp_api\n",
    "from serpapi import GoogleSearch\n",
    "import pprint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregating Google Jobs Data Using SerpAPI.com\n",
    "The data for this project is obtained from SerpAPI.com's Google Jobs API, which is documented here: https://serpapi.com/google-jobs-api.\n",
    "\n",
    "To facilitate daily searches, I have created functions that allow for the search of either a single page with 10 job listings or up to 60 job listings. These functions take a job title string as an argument and can be easily modified to accommodate different job titles for daily searches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function creates one page of Google job results from the Google Jobs API endpoint for the job_title arguement.\n",
    "# The 'uule' setting is to capture listings for remote work.\n",
    "\n",
    "def search_ten_and_create_df(job_title):\n",
    "    params= {\n",
    "        \"q\": f'{job_title}',\n",
    "        \"ibp\": \"htl;jobs\",\n",
    "        \"uule\": \"w+CAIQICINVW5pdGVkIFN0YXRlcw\",\n",
    "        \"hl\": \"en\",\n",
    "        \"gl\": \"us\",\n",
    "        \"ltype\": \"1\",\n",
    "        \"api_key\":f'{serp_api}',\n",
    "        \"engine\":\"google_jobs\"\n",
    "    }\n",
    "\n",
    "    search = GoogleSearch(params)\n",
    "    results = search.get_dict()\n",
    "    jobs_results = results[\"jobs_results\"]\n",
    "\n",
    "    job_data = []\n",
    "    for i in range(len(jobs_results)):\n",
    "        job_data.append({\n",
    "        'job title': jobs_results[i]['title'],\n",
    "        'company name': jobs_results[i]['company_name'],\n",
    "        'job description': jobs_results[i]['description'],\n",
    "        'via': jobs_results[i]['via'],\n",
    "        'job_id': jobs_results[i]['job_id'],\n",
    "        'start':\"0\"\n",
    "        })\n",
    "\n",
    "    ten_jobs_df = pd.DataFrame(job_data)\n",
    "\n",
    "     # Adding the current date and time to track whent he listing was captured in my data\n",
    "    current_date_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    ten_jobs_df['report_run_datetime'] = current_date_time\n",
    "\n",
    "    return ten_jobs_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://serpapi.com/search\n"
     ]
    }
   ],
   "source": [
    "# Creating a DataFrame using the search_ten function for the job title 'Data Analyst'.\n",
    "small_result_df = search_ten_and_create_df('data analyst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function creates 6 pages of Google job results from the Google Jobs API endpoint for the job_title arguement. \n",
    "# The 'uule' setting is to capture remote only listings\n",
    "\n",
    "def search_sixty_and_create_df(job_title):\n",
    "    all_jobs_data = []\n",
    "    start_values = [0, 10, 20, 30, 40, 50]\n",
    "    \n",
    "    for start in start_values:\n",
    "        params= {\n",
    "            \"q\": f'{job_title}',\n",
    "            \"ibp\": \"htl;jobs\",\n",
    "            \"uule\": \"w+CAIQICINVW5pdGVkIFN0YXRlcw\",\n",
    "            \"hl\": \"en\",\n",
    "            \"gl\": \"us\",\n",
    "            \"ltype\": \"1\",\n",
    "            \"api_key\":f'{serp_api}',\n",
    "            \"engine\":\"google_jobs\",\n",
    "            \"start\": f'{start}'\n",
    "        }\n",
    "\n",
    "        search = GoogleSearch(params)\n",
    "        results = search.get_dict()\n",
    "        jobs_results = results[\"jobs_results\"]\n",
    "        \n",
    "        # Using the get method on the dictionary, which returns None if the key is not present in the dictionary.\n",
    "        # The [{}] syntax creates an empty dictionary within a list\n",
    "        jobs_data = []\n",
    "        for i in range(len(jobs_results)):\n",
    "            jobs_data.append({\n",
    "            'job title': jobs_results[i].get('title', None),\n",
    "            'company name': jobs_results[i].get('company_name', None),\n",
    "            'job description': jobs_results[i].get('description', None),\n",
    "            'via': jobs_results[i].get('via', None),\n",
    "            'job_id': jobs_results[i].get('job_id', None),\n",
    "            'posted': jobs_results[i]['detected_extensions'].get('posted_at', None)\n",
    "            })\n",
    "\n",
    "        all_jobs_data.extend(jobs_data)\n",
    "\n",
    "    # Creating a DataFrame from the list of dictionaries\n",
    "    jobs_df = pd.DataFrame(all_jobs_data)\n",
    "    \n",
    "    # Adding the current date and time to track whent he listing was captured in my data\n",
    "    current_date_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    jobs_df['report_run_datetime'] = current_date_time\n",
    "    \n",
    "    return jobs_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n"
     ]
    }
   ],
   "source": [
    "# Creating a 60 listing DataFrame for the job title 'Operations Analyst' using the search_sixty function.\n",
    "oa_google_jobs_df = search_sixty_and_create_df('operations analyst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n"
     ]
    }
   ],
   "source": [
    "# Creating a 60 listing DataFrame for the job title 'Data Analyst' using the search_sixty function.\n",
    "da_google_jobs_df = search_sixty_and_create_df('data analyst')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capturing Details for the Google Jobs Listings\n",
    "\n",
    "Using the SerpAPI's Google Jobs listing endpoint, I have developed a function that takes any number of DataFrames as an argument and captures job listing details such as company rating, number of rating reviews, review source, and salary information. This function enhances the available information in the DataFrames to enable better analysis and insights into the job market."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_job_list_details(*dataframes):\n",
    "    final_df = pd.DataFrame()\n",
    "    \n",
    "    # Looping through each dataframe passed as argument\n",
    "    for dataframe in dataframes:\n",
    "        id_results_list = []\n",
    "        \n",
    "        # Looping through each row of the current dataframe\n",
    "        for index, row in dataframe.iterrows():\n",
    "            job_id = row['job_id']\n",
    "            params = {\n",
    "                \"q\": f\"{job_id}\",\n",
    "                \"api_key\": f\"{serp_api}\",\n",
    "                \"engine\": \"google_jobs_listing\"\n",
    "            }\n",
    "            search = GoogleSearch(params)\n",
    "            results = search.get_dict()\n",
    "            \n",
    "            # Appending the results of the API call to a list\n",
    "            id_results_list.append(results)\n",
    "        \n",
    "        id_jobs_data = []\n",
    "        \n",
    "        # Looping through the list of results and extracting listing information\n",
    "        for i in range(len(id_results_list)):\n",
    "            id_jobs_data.append({\n",
    "            'apply options': id_results_list[i].get('apply_options', [{}])[0].get('link', None),\n",
    "            'rating': id_results_list[i].get('ratings', [{}])[0].get('rating', None),\n",
    "            '# of reviews': id_results_list[i].get('ratings', [{}])[0].get('reviews', None),\n",
    "            'rating source': id_results_list[i].get('ratings', [{}])[0].get('source', None),\n",
    "            'salary based on': id_results_list[i].get('salaries', [{}])[0].get('based_on', None),\n",
    "            'salary from': id_results_list[i].get('salaries', [{}])[0].get('salary_from', None),\n",
    "            'salary to': id_results_list[i].get('salaries', [{}])[0].get('salary_to', None),\n",
    "            'salary source': id_results_list[i].get('salaries', [{}])[0].get('source', None),\n",
    "            'job_id': id_results_list[i].get('search_parameters', [{}]).get('q', None)\n",
    "            })\n",
    "        \n",
    "        # Creating a DataFrame from the list of dictionaries\n",
    "        df = pd.DataFrame(id_jobs_data)\n",
    "        \n",
    "        # Adding the information from the original DataFrame to the new DataFrame\n",
    "        df = pd.concat([dataframe, df], axis=1)\n",
    "        \n",
    "        # Concatenating the current DataFrame to the final DataFrame\n",
    "        final_df = pd.concat([final_df, df], axis=0)\n",
    "        \n",
    "    return final_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n"
     ]
    }
   ],
   "source": [
    "# Pulling the job details for all of the listings captured in the dataframes above\n",
    "all_jobs_3_8_2023_df = get_job_list_details(oa_google_jobs_df, da_google_jobs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the dataframe to a csv for analysis in a seperate notebook\n",
    "all_jobs_3_8_2023_df.to_csv('google_jobs_3_8_2023.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
